# -*- coding: utf-8 -*-
"""Heart failure - with random oversampling+PSO+KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xly38F4frCRmA_cWSsvPhf86qeyxOktr

menggunakan random oversampling + PSO + KNN = 90%
"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# Membaca dataset
df = pd.read_csv('heart_failure.csv')

# cek missing value
pd.isnull(df).sum()

"""Cek distribusi label"""

# Menghitung distribusi label
label_counts = df['DEATH_EVENT'].value_counts()

print(label_counts)
# Membuat plot histogram atau diagram batang
plt.figure(figsize=(10, 6))
label_counts.plot(kind='bar', color='skyblue')
plt.title('Distribusi Label')
plt.xlabel('Label')
plt.ylabel('Jumlah')
plt.show()

"""Balancing Data menggunakan RandomOverSampler"""

from imblearn.over_sampling import RandomOverSampler

# Memisahkan fitur (X) dan label (y)
X = df.drop('DEATH_EVENT', axis=1)
y = df['DEATH_EVENT']

# Membuat instance dari RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Melakukan oversampling pada dataset
X_resampled, y_resampled = ros.fit_resample(X, y)

# Simpan objek RandomOversampling ke dalam file .pkl
with open('ros.pkl', 'wb') as f:
    pickle.dump(ros, f)
    
resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
resampled_df['DEATH_EVENT'] = y_resampled

# Menampilkan distribusi label setelah oversampling
plt.figure(figsize=(8, 5))
pd.Series(y_resampled).value_counts().plot(kind='bar', color='skyblue')
plt.title('Distribusi Label setelah Oversampling')
plt.xlabel('Label')
plt.ylabel('Jumlah')
plt.show()

# cek data setelah di balancing
resampled_df

"""Install library pyswarms untuk PSO"""

pip install pyswarms

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import pyswarms as ps

# Pisahkan fitur dan target (informasi cluster tidak digunakan)
X = resampled_df.drop(['DEATH_EVENT'], axis=1)
y = resampled_df['DEATH_EVENT']

# Split data menjadi train dan test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Feature Selection dengan PSO"""

# Agar angka random yang digenerate tidak berubah ubah
np.random.seed(42)

# Fungsi tujuan untuk PSO (gunakan akurasi sebagai metric)
def fitness_function(position, X_train, X_test, y_train, y_test):
    # Memilih fitur berdasarkan nilai posisi yang lebih besar dari 0.5
    selected_features = position > 0.5  # Sesuaikan threshold dengan kebutuhan
    # Menangani kasus ketika tidak ada fitur yang dipilih
    if not np.any(selected_features):
        return float('inf')  # Set a high cost

    # Membuat model k-Nearest Neighbors dengan jumlah tetangga 3
    clf = KNeighborsClassifier(n_neighbors=3) # KNN digunakan sebagai model klasifikasi untuk mengukur akurasi dari setiap kombinasi fitur yang dipilih oleh PSO

    # Melatih model hanya pada fitur-fitur yang dipilih
    clf.fit(X_train.iloc[:, selected_features], y_train)

    # Melakukan prediksi menggunakan data uji dan fitur yang dipilih
    y_pred = clf.predict(X_test.iloc[:, selected_features])

    # Menghitung akurasi dan mengembalikan nilai negatif akurasi sebagai cost
    return -accuracy_score(y_test, y_pred)  # Minimize the negative of accuracy

# Inisialisasi jumlah partikel dalam algoritma PSO
n_particles = 12

# Inisialisasi jumlah dimensi berdasarkan jumlah fitur dalam data input X
dimensions = X.shape[1]

# Konfigurasi parameter untuk algoritma PSO dalam bentuk kamus
options = {'c1': 0.5, 'c2': 0.5, 'w': 0.5}
# c1 dan c2: Koefisien akselerasi yang memengaruhi kecepatan partikel
# w: Faktor inersia yang mempengaruhi kemampuan partikel untuk menjaga arah dan kecepatan

# Pembuatan objek optimizer menggunakan GlobalBestPSO dari pustaka pyswarms
optimizer = ps.single.GlobalBestPSO(n_particles=n_particles, dimensions=dimensions, options=options)
# n_particles: Jumlah partikel dalam populasi
# dimensions: Jumlah dimensi dalam ruang pencarian
# options: Konfigurasi parameter algoritma PSO
# GlobalBestPSO: Varian PSO di mana partikel terbaik global digunakan untuk memperbarui posisi partikel

# Optimasi
cost, pos = optimizer.optimize(fitness_function, iters=50, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)

# Seleksi fitur berdasarkan posisi terbaik
selected_features = pos > 0.5  # Sesuaikan threshold dengan kebutuhan
print("Selected Features:", X.columns[selected_features])

# Pisahkan fitur dan target
X_selected = resampled_df[X.columns[selected_features]] # fitur yang digunakan hanya fitur yang telah diseleksi sebelumnya
y = resampled_df['DEATH_EVENT']

# Split data menjadi train dan test set (80:20)
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)
X_train

# Normalisasi fitur
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Simpan objek StandardScaler ke dalam file .pkl
with open('scaler.pkl', 'wb') as f1:
    pickle.dump(scaler, f1)
    
print(X_train)

from sklearn.metrics import confusion_matrix,accuracy_score, classification_report

# Inisialisasi dan latih model KNN dengan parameter berikut:
knn_classifier = KNeighborsClassifier(
    n_neighbors=24,  # Ganti dengan jumlah tetangga yang diinginkan
    weights='distance',  # Ganti dengan 'uniform' atau 'distance' sesuai kebutuhan
    algorithm='auto',  # Ganti dengan 'ball_tree', 'kd_tree', atau 'brute'
    leaf_size=5,  # Ganti dengan ukuran daun yang diinginkan
    p=2  # Ganti dengan 1 untuk metrik jarak Manhattan atau 2 untuk Euclidean
)


# Latih model pada data training
model=knn_classifier.fit(X_train, y_train)

# Simpan objek KNearestNeighbor ke dalam file .pkl
with open('knn_model.pkl', 'wb') as f3:
    pickle.dump(model, f3)
    
# Prediksi pada data test
y_pred = knn_classifier.predict(X_test)

# Hitung confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Hitung akurasi
accuracy = accuracy_score(y_test, y_pred)
print("\nAccuracy:", accuracy)

# Tampilkan confusion matrix
print("Confusion Matrix:")
print(cm)

# Tampilkan juga classification report yang menyediakan precision, recall, dan f1-score
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""Visualisasi Confusion Matrix"""

import seaborn as sns

# Visualize confusion matrix with seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Simpan data ke dalam objek
data_to_pickle = {
    'ros' : ros,
    'standarisasi' : scaler,
    'predictions' : model
}

with open('data_file_gab.pkl', 'wb') as f4:
    pickle.dump(data_to_pickle, f4)